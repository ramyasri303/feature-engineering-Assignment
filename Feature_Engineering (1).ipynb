{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "- A parameter is used to describe the entire population being studied. For example,\n",
        "we want to know the average length of a butterfly.\n",
        "This is a parameter because it is states something about the entire population of butterflies."
      ],
      "metadata": {
        "id": "qp8ebhCGUIV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation? What does negative correlation mean?\n",
        "-  A Guide for Beginners | SimplilearnCorrelation, in statistics, is a measure that describes the extent to which two variables change together.\n",
        "A negative correlation is a relationship between two variables that move in opposite directions. In other words, when variable A increases, variable B decreases. A negative correlation is also known as an inverse correlation. Two variables can have varying strengths of negative correlation.\n"
      ],
      "metadata": {
        "id": "7hcgfHIgUSVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "- Machine learning (ML) is a branch of artificial intelligence that enables systems to learn from data and improve their performance on a specific task without explicit programming. It involves algorithms that analyze data, identify patterns, and make predictions or decisions. The main components of machine learning are data, algorithms, models, and predictions.\n"
      ],
      "metadata": {
        "id": "P8kemxH7Vka2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "- Loss value is a crucial metric for evaluating a machine learning model's performance. It quantifies the difference between the model's predictions and the actual values, with lower loss indicating better accuracy. By minimizing the loss during training, models learn to make more accurate predictions. Monitoring loss trends helps identify overfitting or underfitting and guides model"
      ],
      "metadata": {
        "id": "XguhPPpqWEYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "- In data analysis, variables are classified as either continuous or categorical. Continuous variables represent quantities that can be measured and have an infinite number of values between any two given points, such as height or temperature. Categorical variables represent characteristics or qualities that can be grouped into distinct categories, like colors or types of fruit.\n"
      ],
      "metadata": {
        "id": "-72HCnk1WNR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "- Categorical variables, which represent qualitative data, need to be converted into a numerical format before being used in most machine learning algorithms. Common techniques for handling categorical variables include one-hot encoding, label encoding, and ordinal encoding.\n"
      ],
      "metadata": {
        "id": "uzAInp59WWxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "- In the context of machine learning, training and testing a dataset refers to the process of splitting a dataset into two subsets: one used to train a model and the other used to evaluate its performance on unseen data. The training set is used to teach the algorithm patterns and relationships within the data, while the testing set assesses how well the trained model generalizes to new, previously unobserved data.  "
      ],
      "metadata": {
        "id": "PGzPEgkaW0ki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in the scikit-learn library that provides a variety of functions and classes for transforming raw data into a format suitable for machine learning algorithms. This process, known as data preprocessing, is crucial for improving the performance and accuracy of models."
      ],
      "metadata": {
        "id": "W3zTelWkiH1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "- n machine learning, a test set is a portion of a dataset that is separate from the training and validation sets and is used to evaluate the performance of a trained machine learning model. It simulates real-world data that the model will encounter after deployment, providing an unbiased estimate of how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "E9yWdIlPiRsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?\n",
        "- In machine learning, a test set is a portion of a dataset that is separate from the training and validation sets and is used to evaluate the performance of a trained machine learning model. It simulates real-world data that the model will encounter after deployment, providing an unbiased estimate of how well the model generalizes to new, unseen data.\n"
      ],
      "metadata": {
        "id": "DX66zBsTicUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "- In machine learning, a test set is a portion of a dataset that is separate from the training and validation sets and is used to evaluate the performance of a trained machine learning model. It simulates real-world data that the model will encounter after deployment, providing an unbiased estimate of how well the model generalizes to new, unseen data."
      ],
      "metadata": {
        "id": "6IzGtY2Wi1Mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "- Correlation, in statistics, is a measure that describes the extent to which two variables change together. It indicates how strongly they are related, and whether that relationship is positive (both increase or decrease together) or negative (one increases as the other decreases). It's important to remember that correlation does not imply causation; just because two things are correlated doesn't mean one causes the other.\n",
        "Here's a more detailed explanation:\n",
        "Relationship, not causation:\n",
        "Correlation simply shows a statistical relationship between variables. It doesn't explain why the variables are related or whether one variable causes a change in the other.\n",
        "Linear relationship:\n",
        "Correlation is often used to describe linear relationships, meaning the relationship can be represented by a straight line on a graph.\n",
        "Strength of correlation:\n",
        "Correlation coefficients (like Pearson's r) range from -1 to +1. A value of +1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation, and 0 indicates no correlation.\n",
        "Examples:\n",
        "Positive correlation: Height and weight tend to be positively correlated; taller people are usually heavier.\n",
        "Negative correlation: The number of hours spent studying and the number of errors on a test tend to be negatively correlated.\n",
        "No correlation: There's likely no correlation between shoe size and intelligence.\n",
        "Importance:\n",
        "Correlation is a valuable tool for understanding relationships in data, but it's crucial to interpret it with caution and not automatically assume causation.\n",
        "Correlation | Introduction to Statistics - JMP\n",
        "What is correlation? Correlation is a statistical measure that expresses the extent to which two variables are linearly related (m...\n",
        "\n",
        "JMP Statistical Discovery\n",
        "Correlation: Meaning, Strength, and Examples - Verywell Mind\n",
        "30 Nov 2023 â€” A correlation means that there is a relationship between two or more variables.\n"
      ],
      "metadata": {
        "id": "hILytCb5jCzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "- A negative correlation, also known as an inverse correlation, means that two variables tend to move in opposite directions: when one variable increases, the other tends to decrease, and vice versa. This relationship is not always perfect, but the general trend is for the variables to move in opposite directions."
      ],
      "metadata": {
        "id": "4HmrIKzXjYhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "- The correlation coefficient is determined by dividing the covariance by the product of the two variables' standard deviations. Standard deviation is a measure of the dispersion of data from its average. Covariance is a measure of how two variables change together"
      ],
      "metadata": {
        "id": "R2xkKY1yjla3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "- Causation means that one event is the direct result of another event, while correlation means two events are related but not necessarily in a cause-and-effect way"
      ],
      "metadata": {
        "id": "6y_spEh3jzbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "- In machine learning, an optimizer is an algorithm that adjusts the parameters of a model, like weights and biases, to minimize the loss function during training. This process helps the model learn from data and improve its accuracy. Different optimizers use various strategies to update these parameters, impacting training speed and model performance"
      ],
      "metadata": {
        "id": "0Z9KcwQukCAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "- sklearn.linear_model is a module within the scikit-learn (sklearn) library in Python. It provides a collection of linear models for regression and classification tasks. Linear models are a fundamental part of machine learning, using a linear combination of input features to predict the target variable.\n"
      ],
      "metadata": {
        "id": "lx6tEh3vkNCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "- The model.fit() function is a crucial part of training machine learning models. It's the process where the model learns from the provided data, adjusting its internal parameters (weights and biases) to minimize errors and improve predictive accuracy.\n"
      ],
      "metadata": {
        "id": "pGCUBYnakbq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "- Purpose : model. predict() is used to generate predictions from the trained model based on new input data. It does not require true labels and does not compute any metrics."
      ],
      "metadata": {
        "id": "cLwINPpTkn_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "- In data analysis, variables are classified as either continuous or categorical. Continuous variables represent quantities that can be measured and have an infinite number of values between any two given points, such as height or temperature. Categorical variables represent characteristics or qualities that can be grouped into distinct categories, like colors or types of fruit."
      ],
      "metadata": {
        "id": "M_MIEOkOlNLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "- Feature scaling, also known as normalization or standardization, is a crucial data preprocessing step in machine learning that transforms numerical features to a common scale, typically between 0 and 1 or with a mean of 0 and a standard deviation of 1. This process helps algorithms that are sensitive to the scale of input data, like k-nearest neighbors, support vector machines, and neural networks, to learn more effectively and accurately."
      ],
      "metadata": {
        "id": "mwOQX2JglYGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "- Feature scaling is a crucial preprocessing step in machine learning to normalize the range of independent variables or features of data. It is performed to ensure that all features contribute equally to the model, preventing features with larger values from dominating those with smaller values. Here's how to perform scaling in Python:\n"
      ],
      "metadata": {
        "id": "ZosseAJmlhVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is sklearn.preprocessing?\n",
        "- The sklearn.preprocessing module in scikit-learn provides a set of tools for transforming raw input data into a format that is more suitable for machine learning algorithms. This is a crucial step in the machine learning pipeline, as many algorithms perform better when data is standardized or normalized.\n",
        "Here are some common preprocessing techniques available in sklearn.preprocessing:\n",
        "Standardization:\n",
        "Scales features to have zero mean and unit variance. This is done using StandardScaler.\n",
        "Min-Max Scaling:\n",
        "Scales features to a given range, typically between 0 and 1. This is done using MinMaxScaler.\n",
        "Normalization:\n",
        "Scales features to have a unit norm. This is done using Normalizer.\n",
        "Encoding Categorical Features:\n",
        "Converts categorical data into numerical data. Common methods include one-hot encoding using OneHotEncoder and label encoding using LabelEncoder.\n",
        "Imputation:\n",
        "Handles missing values by replacing them with estimated values. This is done using SimpleImputer.\n",
        "Polynomial Features:\n",
        "Generates polynomial and interaction features from existing features using PolynomialFeatures.\n",
        "Preprocessing data with sklearn.preprocessing helps to improve the performance and stability of machine learning models by ensuring that the data is in a suitable format for the learning algorithm."
      ],
      "metadata": {
        "id": "Qob5EZK2luek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How do we split data for model fitting (training and testing) in Python?\n",
        "- o split data for model fitting, training, and testing in Python, the train_test_split function from the sklearn.model_selection module is commonly used. This function divides the dataset into a training set (used to train the model) and a testing set (used to evaluate the trained model's performance on unseen data)."
      ],
      "metadata": {
        "id": "TQB0DPS2mUBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "-   Data encoding is the process of converting information from one format to another, often for storage, transmission, or analysis. It's like translating data into a different language that computers can understand and process efficiently. This process is essential for ensuring data can be shared, accessed, and interpreted correctly, especially across different systems and applications"
      ],
      "metadata": {
        "id": "L6inKXv2mdBg"
      }
    }
  ]
}